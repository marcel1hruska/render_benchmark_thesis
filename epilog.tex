\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Both ART and Mitsuba2 display several unique properties and implementation details, such us custom spectral sampling techniques. Despite that, we have shown that it is possible to methodically test the appearance computations of the distinct renderers. Each tested phenomenon has a properly defined physical model, which allows us to expose some of the exact aspects of its computations and to evaluate its functionality.

We have successfully created evaluation scenes that focus on the following scenarios: polarization, GGX reflectance, iridescence, fluorescence and overall spectral accuracy. Each of these scenarios contains a small number of scenes that can be rendered in a short amount of time while they, to our best knowledge, demonstrate all essential aspects of the tested feature. Unfortunately, dispersion had to be omitted and left for future work, due to its unverified implementation.

The straightforward descriptions of the scenes, their basic geometry along with their in-depth documentation should be enough to comprehend their purpose in the evaluation process. As an addition, the benchmark contains various data to help a potential user to extend the evaluation suite or his rendering system coherently and correctly. These include code snippets, unified geometry, and spectral values for various colors that are used in the scenes.

Even though we do not include an absolute metric that would explicitly determine the correctness of individual computations, the reference images along with the difference images provide enough information for a reasonably skilled user to assess the accuracy by himself.

\section{Future Work}

Despite the various functionalities that the benchmark has, there are several possible extensions that we consider interesting or useful. However, they were not essential for the purposes of this thesis and we purposely avoided them. Providing more time, these would be fine assets to the benchmark, further extending its capabilities and effectiveness.

\begin{description}
	\item[Enhanced results] Right now, the results visualizer consists of a very basic UI where the user may look at the images and interact with them. Several features could be implemented, e.g. performance counter, comments explaining each scene, highlights of the scene, etc.
	\item[Dispersion] As the dispersion is the only phenomenon that we have talked about but have not evaluated, it would be appropriate to add it to the benchmark as soon as the implementation for Mitsuba2 and/or ART is fully functional.
	\item[More renderers] The addition of multiple renderers heavily depends on the supported features of the specific renderer and the interest of its developers. More renderers provide more cross references, which could potentially expose previously hidden inaccuracies and consequently improve the effectiveness of the whole evaluation framework. 
	\item[Common scene format] Including more renderers would be significantly simplified by describing the scenes in a common scene format (e.g. Universal Scene Description by~\citet{usdDoc}). This approach would, of course, need a conversion tool from the universal format to the renderer-specific one.
\end{description}